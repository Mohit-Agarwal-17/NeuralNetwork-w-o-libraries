{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer init\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # init wts and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # inputs init\n",
    "        self.inputs= inputs\n",
    "        # calulating the forward pass\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward Pass\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # gradient on values to be passed to prev layer\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# ReLU Activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # ReLU gives max of 0, x. Inputs is the output of the first layer and np.maximum performs this for every value in inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # since we need to modify dvalues itself we make a copy\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # for values <= 0 the value will be 0\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax Activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # exp(x) / sum(exp(x))\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) #subtraction is done to ensure there is no explosion of bigger values\n",
    "        probablitites = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probablitites\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "# Loss\n",
    "class Loss:\n",
    "    # calculate loss from y and output\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # apply forward function\n",
    "        sample_losses = self.forward(output, y)\n",
    "        print(\"Loss: \", sample_losses)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        mean_loss = np.mean(sample_losses)\n",
    "\n",
    "        return mean_loss\n",
    "    \n",
    "# Cross Entropy Loss\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # No. of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip data from both sides to prevent change in mean\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        # Only category labels\n",
    "        if(len(y_true.shape) == 1):\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Y_true is one hot encoded\n",
    "        if(len(y_true.shape) == 2):\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # loss\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # no. of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # no. of labels\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape) == 1):\n",
    "            # converting to one hot\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # derivative\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # normalising\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create a dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create the ReLU Actuvation \n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create the 2nd dense layer with 3 input features as the previous layer gave 3 outputs ans 3 output as we have 3 classes\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# # Create the Softmax Activation\n",
    "# activation2 = Activation_Softmax()\n",
    "\n",
    "# # Create the loss function\n",
    "# loss_function= Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [1.0986123 1.0986127 1.0986136 1.0986145 1.0986153 1.0986137 1.0986168\n",
      " 1.098617  1.0986178 1.0986192 1.0986199 1.098619  1.0986207 1.0986221\n",
      " 1.098622  1.0986223 1.0986185 1.0986241 1.0986226 1.098626  1.098626\n",
      " 1.0986195 1.098617  1.0986255 1.0986354 1.0986278 1.0986179 1.0986184\n",
      " 1.0986398 1.0986419 1.0986304 1.098637  1.0986211 1.0986254 1.0986376\n",
      " 1.0986471 1.0986526 1.098653  1.0986507 1.098654  1.0986497 1.0986483\n",
      " 1.0986478 1.0986323 1.0986618 1.098662  1.098664  1.0986414 1.0986664\n",
      " 1.0986533 1.0986621 1.0986348 1.0986497 1.0986599 1.0986315 1.0986141\n",
      " 1.0986186 1.0986125 1.0986273 1.0986137 1.0986171 1.098616  1.0986123\n",
      " 1.0986311 1.0986419 1.0986344 1.0986123 1.0986576 1.0986377 1.0986589\n",
      " 1.0986654 1.0986648 1.0986633 1.0986542 1.0986674 1.0986677 1.0986686\n",
      " 1.0986708 1.0986696 1.0986639 1.0986731 1.0986443 1.0986497 1.0986748\n",
      " 1.0986497 1.0986748 1.0986547 1.0986434 1.0986588 1.0986766 1.0986344\n",
      " 1.0986977 1.0986686 1.0987039 1.0986919 1.098709  1.0986958 1.0987172\n",
      " 1.0987127 1.098722  1.0986123 1.0986128 1.0986135 1.0986153 1.0986153\n",
      " 1.0986152 1.0986183 1.0986192 1.0986147 1.0986161 1.0986137 1.0986179\n",
      " 1.0986241 1.098613  1.0986227 1.0986149 1.0986123 1.0986197 1.0986123\n",
      " 1.0986149 1.0986127 1.0986228 1.0986152 1.0986241 1.0986125 1.098617\n",
      " 1.0986165 1.0986314 1.0986289 1.0986291 1.0986217 1.098633  1.0986272\n",
      " 1.098634  1.0986317 1.0986371 1.0986376 1.0986385 1.0986384 1.0986383\n",
      " 1.0986378 1.0986364 1.0986375 1.0986311 1.0986297 1.0986218 1.0986421\n",
      " 1.0986165 1.0986434 1.0986321 1.0986229 1.0986485 1.098628  1.0986263\n",
      " 1.0986216 1.098616  1.0986595 1.0986164 1.098617  1.0986412 1.0986454\n",
      " 1.098671  1.098675  1.0986711 1.0986497 1.0986769 1.098666  1.098659\n",
      " 1.0986668 1.0986786 1.0986652 1.0986745 1.0986578 1.0986841 1.0986865\n",
      " 1.0986457 1.0986809 1.0986754 1.0986645 1.0986422 1.0986489 1.0986755\n",
      " 1.0986456 1.0986873 1.0986316 1.09866   1.0986328 1.0986334 1.0986155\n",
      " 1.0986733 1.0986123 1.0986614 1.0986596 1.0986346 1.0986767 1.0986708\n",
      " 1.0986807 1.0986805 1.0986531 1.0986693 1.0986123 1.0986109 1.09861\n",
      " 1.0986086 1.0986078 1.0986054 1.0986048 1.0986024 1.0986047 1.098603\n",
      " 1.098609  1.0986058 1.0986083 1.0986017 1.0985988 1.0986042 1.0986068\n",
      " 1.0985962 1.0985739 1.0985948 1.0986065 1.0985909 1.0985881 1.0985681\n",
      " 1.098602  1.0985644 1.0985594 1.0985554 1.0985595 1.0985518 1.0985633\n",
      " 1.0985466 1.098544  1.0985512 1.0985405 1.0985653 1.098593  1.0985372\n",
      " 1.0985689 1.0985835 1.098547  1.0985583 1.0985734 1.0985739 1.0985808\n",
      " 1.0985378 1.0985814 1.0985769 1.098592  1.0985706 1.0985731 1.0986037\n",
      " 1.0985707 1.0985788 1.0985432 1.0985382 1.0985696 1.0985863 1.0985857\n",
      " 1.0985255 1.0985543 1.0985299 1.0985267 1.0985194 1.0985484 1.098522\n",
      " 1.0985324 1.0985252 1.0985198 1.0985336 1.0985485 1.0985223 1.0985363\n",
      " 1.0985707 1.098536  1.0985528 1.0985816 1.0985836 1.098514  1.0984505\n",
      " 1.098563  1.0984706 1.0985769 1.0984368 1.0984722 1.098446  1.0985355\n",
      " 1.098427  1.0984247 1.0984327 1.098446  1.0984297 1.0985248 1.0984324\n",
      " 1.0984129 1.0984786 1.0984266 1.0984656 1.0984188 1.098476 ]\n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "dense1.forward(X)\n",
    "# print(X.shape)\n",
    "# Apply the first activation to the ouput of 1st layer\n",
    "activation1.forward(dense1.output)\n",
    "# print(dense1.output.shape)\n",
    "# Forward pass to the 2nd layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# # Applying the 2nd activation\n",
    "# activation2.forward(dense2.output)\n",
    "\n",
    "# print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0986104\n"
     ]
    }
   ],
   "source": [
    "# Print loss value\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
