{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer init\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # init wts and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # calulating the forward pass\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU Activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # ReLU gives max of 0, x. Inputs is the output of the first layer and np.maximum performs this for every value in inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax Activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # exp(x) / sum(exp(x))\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) #subtraction is done to ensure there is no explosion of bigger values\n",
    "        probablitites = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probablitites\n",
    "\n",
    "\n",
    "# Loss\n",
    "\n",
    "class Loss:\n",
    "    # calculate loss from y and output\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # apply forward function\n",
    "        sample_losses = self.forward(output, y)\n",
    "        print(\"Loss: \", sample_losses)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        mean_loss = np.mean(sample_losses)\n",
    "\n",
    "        return mean_loss\n",
    "    \n",
    "# Cross Entropy Loss\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # No. of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip data from both sides to prevent change in mean\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        # Only category labels\n",
    "        if(len(y_true.shape) == 1):\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Y_true is one hot encoded\n",
    "        if(len(y_true.shape) == 2):\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # loss\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihoods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create a dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create the ReLU Actuvation \n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create the 2nd dense layer with 3 input features as the previous layer gave 3 outputs ans 3 output as we have 3 classes\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create the Softmax Activation\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create the loss function\n",
    "loss_function= Loss_CategoricalCrossEntropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.3333333  0.3333333 ]\n",
      " [0.33333433 0.333333   0.33333272]\n",
      " [0.3333336  0.33333206 0.33333433]\n",
      " [0.33333465 0.3333328  0.3333325 ]]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "dense1.forward(X)\n",
    "# print(X.shape)\n",
    "# Apply the first activation to the ouput of 1st layer\n",
    "activation1.forward(dense1.output)\n",
    "# print(dense1.output.shape)\n",
    "# Forward pass to the 2nd layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Applying the 2nd activation\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [1.0986123 1.0986123 1.0986093 1.0986115 1.0986084 1.0986072 1.0986013\n",
      " 1.0986123 1.098603  1.0985992 1.0986001 1.0985905 1.0985931 1.0985918\n",
      " 1.0985854 1.0985839 1.0985813 1.0985827 1.0985768 1.0985798 1.0985814\n",
      " 1.0985931 1.0985707 1.0985677 1.0985938 1.0985674 1.0986011 1.0986075\n",
      " 1.0985999 1.0985847 1.098599  1.0986055 1.098607  1.0985966 1.0985916\n",
      " 1.0985959 1.0986013 1.0986038 1.098576  1.0986006 1.0985991 1.0985937\n",
      " 1.0985923 1.098598  1.0985928 1.0985994 1.0985904 1.0986005 1.0985914\n",
      " 1.0986089 1.0986013 1.0985883 1.098594  1.0986071 1.0985973 1.0985985\n",
      " 1.0986123 1.098595  1.0986062 1.0986123 1.0986067 1.0986123 1.0985811\n",
      " 1.0985891 1.0986114 1.0986099 1.0986123 1.0985636 1.0985603 1.0985905\n",
      " 1.0985609 1.0986097 1.0984857 1.0984936 1.0985482 1.0984738 1.0984726\n",
      " 1.0984826 1.0984821 1.0984572 1.0984572 1.0984572 1.0985271 1.0984677\n",
      " 1.0984459 1.0984439 1.0984418 1.0984397 1.0985104 1.0984614 1.0985291\n",
      " 1.0984967 1.0985051 1.0985436 1.0985863 1.0985788 1.0985844 1.0985649\n",
      " 1.0985732 1.0985699 1.0986123 1.0986135 1.098617  1.0986192 1.0986142\n",
      " 1.0986241 1.0986183 1.0986285 1.0986313 1.0986285 1.0986358 1.0986382\n",
      " 1.0986391 1.0986277 1.0986391 1.0986403 1.0986263 1.0986332 1.0986346\n",
      " 1.0986123 1.098614  1.0986123 1.098627  1.098619  1.0986183 1.0986161\n",
      " 1.0986333 1.0986147 1.0986123 1.0986161 1.0986227 1.0986238 1.0986199\n",
      " 1.0986364 1.0986123 1.0986373 1.098613  1.098639  1.0986313 1.0986416\n",
      " 1.0986422 1.0986434 1.0986401 1.098644  1.0986441 1.0986407 1.0986745\n",
      " 1.0986415 1.0986291 1.0986494 1.09865   1.0986412 1.0986485 1.0986825\n",
      " 1.0986345 1.0987008 1.0987161 1.0986507 1.0986907 1.0986702 1.098717\n",
      " 1.0987258 1.0987068 1.0987465 1.0987598 1.0987651 1.0987645 1.0987669\n",
      " 1.0987346 1.0987458 1.0987768 1.0987527 1.0987806 1.0987151 1.09865\n",
      " 1.0987809 1.0986404 1.098764  1.0987422 1.0987368 1.0986172 1.0987964\n",
      " 1.0986388 1.0987448 1.0986123 1.0986373 1.0986123 1.0986133 1.0986212\n",
      " 1.0986243 1.0986652 1.0986733 1.0986564 1.0986444 1.0986338 1.0986352\n",
      " 1.0986853 1.098686  1.098671  1.0986309 1.0986123 1.098613  1.0986146\n",
      " 1.0986155 1.0986167 1.0986183 1.0986178 1.0986205 1.0986218 1.0986229\n",
      " 1.0986245 1.0986218 1.0986246 1.0986229 1.0986235 1.0986152 1.0986273\n",
      " 1.0986233 1.0985838 1.0986252 1.098614  1.0986176 1.0985867 1.0985744\n",
      " 1.098595  1.0985886 1.0985804 1.0985641 1.0985792 1.0985672 1.0986184\n",
      " 1.0985764 1.0985525 1.0985551 1.0985494 1.0985503 1.0985498 1.0985495\n",
      " 1.0985442 1.0985798 1.098555  1.0985343 1.0985389 1.0985459 1.0985484\n",
      " 1.0985351 1.0985547 1.0985677 1.0985738 1.0986283 1.0986123 1.0985863\n",
      " 1.0986123 1.0986109 1.0986165 1.0986581 1.0986476 1.0986818 1.0986633\n",
      " 1.0986309 1.0986813 1.0986865 1.0986882 1.0986542 1.0986838 1.0986879\n",
      " 1.0986567 1.0986907 1.0986782 1.0986928 1.0986953 1.0986605 1.0986953\n",
      " 1.098701  1.098702  1.098674  1.0986732 1.098558  1.0986276 1.0986993\n",
      " 1.098669  1.0985193 1.0986524 1.0984663 1.0986569 1.0984962 1.0985954\n",
      " 1.0985383 1.0984484 1.0984843 1.0984429 1.0984432 1.0984374 1.0984759\n",
      " 1.0984395 1.0984342 1.098442  1.0985084 1.0984275 1.0984282]\n",
      "1.0986074\n"
     ]
    }
   ],
   "source": [
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
